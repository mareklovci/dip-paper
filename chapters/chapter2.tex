\chapter{Methodology}\label{ch:methodology}

Data Quality methodology consists of two main parts, the model and the metamodel.
The model defines activities, their description, goals and sequence of order needed for extraction of Data Quality metrics.
The metamodel defines components of DQ process, maps the relationship among the components and activities respectively.

\section{Business Problems and Data Defects}

% Není možné mluvit o kvalitě dat a metodikách pro její zajištění, bez zmínky o konkrétních typech problémů, které se v rámci tématu vyskytují.
It is not possible to talk about data quality and methodologies for ensuring it, without mentioning the specific types of problems that occur within the topic.
There is quite a~few common defects in the field of data engineering and data science.

\subsection*{Missing Data}

This is data that does not reach the destination data store.
This problem usually occurs when handling the data needed to clean up in the source database; by operating with invalid or incorrect lookup table in the transformation logic; or by invalid table joins.
An example of missing data is shown in Figure~\ref{fig:missing-data}.

\paragraph*{Example} We transform data from Task Management Solution.
Lookup table should contain a field value of \enquote{Minor} which maps to \enquote{Low}.
However, source data field contains \enquote{Mino} - missing the \textit{r} and fails the lookup, resulting in the target data field containing null.
If this occurs on a key field, a possible join would be missed and the entire row could fall out.

\begin{figure}[htb]
    \centering
    \includegraphics[width=.5\textwidth]{figures/missing-data.png}
    \caption{Missing Data}
    \label{fig:missing-data}
\end{figure}
\FloatBarrier

\subsection*{Truncation of Data}

Many data is being lost by truncation of the data fields.
This happens when there are invalid field lengths on target database or by transformation logic not taking into account field lengths from the source.

\paragraph*{Example} We transform financial data with complete exchange-traded fund (ETF) names.
Source field value \enquote{iShares Global High Yield Corp Bond UCITS ETF} is being truncated to \textit{varchar(32)}.
since the source data field did not have the correct length to capture the entire field, only \enquote{iShares Global High Yield Corp B} is stored.

% \begin{figure}[htb]
%     \centering
%     \includegraphics[width=.5\textwidth]{figures/truncation-data.png}
%     \caption{Data Truncation}
%     \label{fig:truncation-data}
% \end{figure}
% \FloatBarrier

\subsection*{Data Type Mismatch}

Data types not setup correctly on target database cause serious problems.
This usually happens when using ETL pipeline with an automatic or semi-automatic column type recognition.
% Data inženýr spoléhá na bezchybné rozpoznání datového typu a nezkontroluje správnost výstupních tabulek.
The data engineer relies on error-free data type recognition and does not check the accuracy of the output tables.

\paragraph*{Example} Source data field was required to be a \textit{varchar}, however, when initially configured, was setup as a \textit{date}.

% \begin{figure}[htb]
%     \centering
%     \includegraphics[width=.5\textwidth]{figures/mismatch-data.png}
%     \caption{Data Type Mismatch}
%     \label{fig:mismatch-data}
% \end{figure}
% \FloatBarrier

\subsection*{Null Translation}

In the source dataset, \textit{null} values are not being transformed to correct target values.
Development team did not include the \textit{null} translation in the ETL process.

\paragraph*{Example} A \enquote{Null} source data field was supposed to be transformed to \enquote{None} in the target data field.
However, the logic was not implemented, resulting in the target data field containing \enquote{null} values\footnotemark.

\footnotetext{
    None is a concept that describes the absence of anything at all (nothingness), while Null means \textit{unknown} (we do not know if there is a value or not).
}

% \begin{figure}[htb]
%     \centering
%     \includegraphics[width=.5\textwidth]{figures/null-translation.png}
%     \caption{Null Translation}
%     \label{fig:null-translation}
% \end{figure}
% \FloatBarrier

\subsection*{Wrong Translation}

Wrong translations happen when a source data field for \textit{null} was supposed to be transformed to \enquote{None} in the target data field, but was not transformed correctly.
The logic was not implemented, resulting in the target data field containing \textit{null} values.
Wrong translation is the exact opposite to \textit{Null Translation}.

\paragraph*{Example} Target field should only be populated when the source field contains certain values, otherwise should be set to null.
Let's look at a very basic example.
During analytical processing of medical data (e.g., list of patients with oncological finding), we need to set target field to \textit{true} if the one or multiple source values indicate certain treatment.
However, the target field is populated (either with blank charater or other values) although source values do not correspond to the required logic.

% \begin{figure}[htb]
%     \centering
%     \includegraphics[width=.5\textwidth]{figures/wrong-translation.png}
%     \caption{Wrong Translation}
%     \label{fig:wrong-translation}
% \end{figure}
% \FloatBarrier

\subsection*{Misplaced Data}

If the source data fields are not being transformed to the correct target data fields, we call the issue \enquote{Misplaced Data}.
One of the possible causes is that development team inadvertently mapped the source data field to the wrong target data field.

\paragraph*{Example} A source data field was supposed to be transformed to target data field \enquote{Last\_Update}.
However, the development team inadvertently mapped the source data field to \enquote{Date\_Created}.

% \begin{figure}[htb]
%     \centering
%     \includegraphics[width=.5\textwidth]{figures/misplaced-data.png}
%     \caption{Misplaced Data}
%     \label{fig:misplaced-data}
% \end{figure}
% \FloatBarrier

\subsection*{Extra Records}

Records which should be excluded in the ETL are included in the ETL.
This happens when developers do not include filter in their code.

\paragraph*{Example} If a record has the deleted field populated, the record and any data related to that record should not be in any ETL.

% \begin{figure}[htb]
%     \centering
%     \includegraphics[width=.5\textwidth]{figures/extra-records.png}
%     \caption{Extra Records}
%     \label{fig:extra-records}
% \end{figure}
% \FloatBarrier

\subsection*{Not Enough Records}

Records which should be in the ETL are not included in the ETL.
Development team had a filter in their code which should not have been there.

\paragraph*{Example} If a record was in a certain state, it should be sent through ETL pipeline over to the data warehouse.

% \begin{figure}[htb]
%     \centering
%     \includegraphics[width=.5\textwidth]{figures/notenough-records.png}
%     \caption{Not Enough Records}
%     \label{fig:notenough-records}
% \end{figure}
% \FloatBarrier

\subsection*{Transformation Logic Errors}

Testing sometimes can lead to finding \enquote{holes} in the transformation logic or realizing the logic is unclear.

Sometimes, the processes are just way too complicated and development team does not take into account special cases.
Most cases fall into a certain branch of logic for a transformation, but a small subset of cases (sometimes with unusual data) may not fall into any branches.
How the analytics and developers handles these cases could be different (and may both end up being wrong) and the logic is changed to accommodate the cases.
The next reason why this happens is that analytic and developer have different interpretation of transformation logic, which results in different values.
This leads to the logic being re-written to become clearer.

\paragraph*{Example} Foreign country cities that contain special language specific characters might need to be dealt with in the ETL code (e.g., Århus).

% \begin{figure}[htb]
%     \centering
%     \includegraphics[width=.5\textwidth]{figures/logic-errors.png}
%     \caption{Transformation Logic Errors}
%     \label{fig:logic-errors}
% \end{figure}
% \FloatBarrier

\subsection*{Simple and Small Errors}

Capitalization, spacing and other small errors cause problems with data.
Data inconsistencies are easy to fix, but happen often.
The only real solution is to always double check data and ETL procedure.

% \begin{figure}[htb]
%     \centering
%     \includegraphics[width=.5\textwidth]{figures/small-errors.png}
%     \caption{Simple and Small Errors}
%     \label{fig:small-errors}
% \end{figure}
% \FloatBarrier

\subsection*{Sequence Generator}

Ensuring that the sequence number of reports are in the correct order is very important when processing follow up reports or answering to an audit.
If the sequence generator is not configured correctly, procedure results in records with a duplicate sequence number.

\paragraph*{Example} Duplicate records in the sales report were doubling up several sales transactions which skewed the report significantly.

% \begin{figure}[htb]
%     \centering
%     \includegraphics[width=.5\textwidth]{figures/sequence-generator-error.png}
%     \caption{Sequence Generator Error}
%     \label{fig:sequence-generator-error}
% \end{figure}
% \FloatBarrier

\subsection*{Undocumented Requirements}

During ETL development, sometimes certain requirements are found, that are \enquote{understood} but are not actually documented anywhere.
This causes issues when members of the development team do not understand or misunderstood the undocumented requirements.

\paragraph*{Example} ETL pipeline contains a restriction in the \enquote{where} clause, limiting how certain reports are brought over.
Moreover, there were used mappings that were understood to be necessary, but were not actually in the requirements.
Occasionally, it turns out that the understood requirements are not what the business wanted.

\subsection*{Duplicate Records}

Duplicate records are two or more records that contain the same data.
This issue happens when development team does not add the appropriate code to filter out duplicate records or there is some unexpected error in data generators.

\paragraph*{Example} Duplicate records in the sales report were doubling up several sales transactions which skewed the report significantly.

% \begin{figure}[htb]
%     \centering
%     \includegraphics[width=.5\textwidth]{figures/duplicate-records.png}
%     \caption{Duplicate Records}
%     \label{fig:duplicate-records}
% \end{figure}
% \FloatBarrier

\subsection*{Numeric Field Precision}

Numbers that are not formatted to the correct decimal point or not rounded per specifications cause precision problems.
This has several causes, development team rounded the numbers to the wrong decimal point, used wrong rounding type or used wrong data type which lead to faulty rounding.

\paragraph*{Example} The sales data did not contain the correct precision and all sales were being rounded to the whole dollar.

\subsection*{Rejected Rows}

Data rows that get rejected by ETL process due to data issues.
Development team did not take into account data conditions that break the ETL for a particular row.
An example of an ETL process with rejected rows is shown in Figure~\ref{fig:rejected-rows}.

\paragraph*{Example} Missing data rows on the sales table caused major issues with the end of year sales report.

\begin{figure}[htb]
    \centering
    \includegraphics[width=.8\textwidth]{figures/rejected-rows.png}
    \caption{Talend ETL Rejects Rows}
    \label{fig:rejected-rows}
\end{figure}
\FloatBarrier

% a~\cite{web:common-defects}

\newpage
\section{Model}

The methodology has several important components that need to be identified or developed.
The metamodel that covers the required components is as depicted in the Figure~\ref{fig:methodology-metamodel}.
The activities within the process model have a goal to develop those components.

Overall, the methodology consists of two main processes.
The first one is \textbf{Specification Process}.
The goal of this processs is to identify and define context specific ways to measure data quality.
The second one is an \textbf{Execution Process}.
Its main goal is to \textit{collect} and \textit{verify} data with output from \textit{Specification Process} taken into account.

\begin{figure}[htb]
    \centering
    \includegraphics[width=.8\textwidth]{figures/dq-methodology.pdf}
    \caption{Methodology Metamodel}
    \label{fig:methodology-metamodel}
\end{figure}
\FloatBarrier

\subsection{Specification Process}

% Specifikační proces slouží jako nástroj pro definování kvalitativních a kvantifikovatelných požadavků na kvalitu.
The specification process serves as a tool for defining qualitative and quantifiable quality requirements.
This is a key part of the system.
% Je to však také jediná část procesu, která vyžaduje nezbytnou iniciativu analytika či analytického týmu.
However, it is also the only part of the process that requires the necessary initiative of the analyst or analytical team.
Now, we describe each part of the process.

\begin{figure}[htb]
    \centering
    \includegraphics[width=.8\textwidth]{figures/specification-process.pdf}
    \caption{Specification Process}
    \label{fig:specification-process}
\end{figure}
\FloatBarrier

\subsubsection{Identification}

This activity focuses on identification of systems, processes and business schemes generatig data.
By identifying weak points and bottlenecks in those processes, we can find causes of poor data.
Also, we need to identify the subprocesses or activities that are mostly affected by the product data quality.

\subsubsection{Metrics Specification}

The goal of this activity is to identify the process metrics or KPIs.
Measuring data quality is all about understanding what data quality attributes are, and choosing the correct data quality metrics.
A comprehensive list of Data Quality Attributes by Eppler (2006) is available in appendix~\ref{ch:data-quality-attributes}.
Specific attributes will be further discussed in Chapter~\ref{ch:quality-classification-system}.

\subsubsection{Verification}

The last part of current process is verification.
This activity has to ensure that selected metrics are meaningful enough, capturing the actual condition of data.

\subsection{Execution Process}

The second main component is the execution process.
This includes the actual collection and validation of data against the requirements obtained by the analysis from the first process.
Ideally, in a semi-automated information system, this part runs independently, without human intervention.
However, we are aware that in many cases it is not possible to implement a fully automated system, either due to the information complexity of the task or the financial costs of system development.

\begin{figure}[htb]
    \centering
    \includegraphics[width=.8\textwidth]{figures/execution-process.pdf}
    \caption{Execution Process}
    \label{fig:execution-process}
\end{figure}
\FloatBarrier

\subsubsection{Collection}

Data collection is a systematic process of gathering observations or measurements.
Data collector can be either \textit{Information System}, computer program or a human.
Before the beginning of collecting data, we need to consider:

\begin{itemize}
    \item the type of data we will collect;
    \item the methods and procedures we will use to collect, store and process data.
\end{itemize}

\subsubsection{Verification}

In our general case, verification is based on actual reliability of data, computed using DQ metrics.
In other scenarios, the verification could be based on data redundancies, therefore based on the comparison of the collected data from two or more different collectors.
If all data match, the data will be considered as valid.
If not, the data remains invalid until a further collector validates it.

Artificial Intelligence and Machine Learning could be used to further ease and optimize data verification.
Especially when processing image data and data with a high level of abstraction.

\subsubsection{Contract}

The contractual process is a subprocess that has the task of marking data as trustworthy if all the necessary requirements are met.
This is the same concept as the so-called \enquote{smart contracts}.
Smart contracts are essentially blockchain programs that are processed when mandatory conditions are fulfilled.
They are commonly used to simplify agreement implementation so that all parties can be sure of the result instantly, without intermediary intervention or time loss.
This leads to workflow automation, initiating the next step if all conditions have been satisfied.

The contracts work by following simple \enquote{if-then} statements.
This mechanism might include allocation of funds to the appropriate parties, sending notifications, or releasing a ticket.

\subsection{Supporting Techniques}

There are a several data quality rules one can deduce from a Feedback-Control Systems view of information systems reviewed by Orr (1998).

\begin{enumerate}
    \item Unused data cannot remain correct for very long;
    \item data quality in an information system is a function of its use, not its collection;
    \item data quality cannot be better than its most strict use;
    \item data quality problems tend to become worse as the system ages;
    \item the less likely some data attribute is to change, the harder it will be to change it when the time comes;
    \item laws of data quality apply equally to data and metadata~\cite{orr1998}.
\end{enumerate}

To prevent the consequences of these rules and the unauthorized creation of data, we present two additianal concepts.
These concepts should be incorporated into the design of the information system respecting the proposed methodology.

\subsubsection{Proof of Constancy}

Proof of Constant Data, alias Proof of Constancy, is a way to assure a constant accuracy of data.
Data have to be regularly updated to keep the accuracy rate high.
Data accuracy rate will decrease progressively based on a specific time frame basis (e.g., X\% per month).
This percentage is different depending on the type of data.
Data sets more sensitive to changes may see this rate decrease by 5\% to 10\% per month or day depending on the circumstances.
On the other hand, established, well-known sets, will see their rate decrease by 0.1\% per month or even year.
A scale of discount rates will have to be established based on the areas of interest and actual items collected.

\subsubsection{Proof of Trust}

Proof of Trust is an instrument for data collector evaluation.
The collector or generator will get \enquote*{quality score} for his/her or its collection actions.
The more collectors initiate, update and verify data correctly, the higher their \enquote*{quality score} will be.
A higher quality score leads to a higher level of \enquote*{trust}.
On the other hand, incorrect collection leads to a retroactive decrease of the collector's quality score.

\section{Use Cases}

In this part, we will present several use cases to illustrate versatile use of the presented framework.

\subsection{Enterprise Information System}

Enterprises suffer from poor data quality.
We propose, following the methodology, to introduce a central register of data sources.
This central register should be supported by a set of services and a central data repository.

After a thorough analysis of data requirements and their quality, a defined set of metrics and key performance indicators parameterizes the verification chain of activities.
If the predefined quality limit is not met, the data will either be rejected or saved with an error flag.
% Jestliže data splňují požadovanou úrověň chybovosti, projdou kontraktačním procesem a jsou požadována za referenční až do doby, kdy je jejich poslední verze je nařazeným procesem kvalitativně degradována a označena za nedůvěryhodnou.
If the data meets the required level of error, they go through the contracting process and are considered as a reference until their latest version is qualitatively degraded by the ordered process (e.g., Proof of Constancy) and marked as untrusted.

% Penalizací za špatnou kvalitu by bylo automatické hlášení vyššímu managementu společnosti.
A penalty for poor quality would be automatic reporting to the company's senior management.
% Management by následně mohl uvalit na osoby zodpovědné za konkrétní datové sady a datové toky sankce ve formě snížení nebo zrušení osobních odměn.
Management could then impose sanctions on those responsible for specific datasets and data flows in the form of reductions or cancellations of personal rewards.

\subsection{IoT Cluster}

Based on the domain and usage of the IoT devices, the data repository could be either centralized (e.g., nuclear power plant cluster of secondary senzors) or decentralized (e.g., community weather stations).

The verification algorithm would - in this case - consist from two general authorities.
The first authority being \textit{k} nearest neighbours of the same sensors (or IoT devices in general), and the second one being the set of domain rules.
Nearest neighbors provide redundancy by which data can be verified.
And, of course, the data itself must meet the criteria restrictions set by the domain of use.

% Špatná kvalita by vedla ke snížení významnosti senzoru v klusteru, případně jeho dočasnému nebo úplnému vyřazení z provozu.
Poor quality would reduce the importance of the sensor in the cluster, or its temporary or complete decommissioning.
% Tento systém by vytořil i velice účinnou obranou bariéru proti útokům.
This system would also create a very effective defense barrier against attacks, especially against data poisoning.

Data poisoning is a class of attacks on machine learning algorithm where an adversary alters a fraction of the training data in order to impair the intended function of the system.
Objective can be to degrade the overall accuracy of the trained classifier, escaping security detection or to favor one product over the another.
Machine Learning systems are usually retrained after deployment to adapt to changes in input distribution, so data poisoning represents serious danger.

Qualitative degradation of data by Proof of Constancy would not be the so important, because we expect very high update frequency.
However, lower update frequancy of IoT device would suggest an error within a system, which could serve as a warning to network operators about a faulty device.
Data from defective equipment should also not be taken into account in many cases.

\subsection{Open Data Library}

The last Use Case shows usage of completely decentralized solution.
The system would allow those who collect and generate data to be rewarded and data would be accessible for use within a decentralized marketplace.
This decentralized network would democratize access to data while rewarding those who generate it.

Data collection would be done through an application (system) used by a community of collectors who are rewarded for their actions.
This reward is calculated according to a~\enquote*{collection value} price.

The collection value would be calculated using an algorithm that takes into account several criteria, such as:
\begin{itemize}
    \item demand and rarity,
    \item online availability and accessibility,
    \item data licensing market value.
\end{itemize}

Each collector receives a quality score to maintain a high level of reliability.
The verified data is then made accessible (through contracting) on the decentralized marketplace and regularly updated to keep it accurate.

Due to the variety of open data, automation of the verification process is practically impossible.
The data must be verified manually.
Thus, a collector has two functions:
\begin{itemize}
    \item initiate the data collection (input and update data),
    \item verify the data collected (check a collected data not yet verified).
\end{itemize}

The mechanism ensures that reward for data collection is divided between collector and verifier.
For example, the collector, who initiated the data, would obtain 60-80\% of the reward.
The verifier would obtain remaining 20-40\%.

Depending on the data, a decentralized network like filecoin could be used as data storage.
Using the blockchain makes the data unalterable, guaranteeing the transparency and traceability of its validation process (collection, verification, update).
The blockchain (tamper-proof, immutable and decentralized) ensures the integrity and verification of the data available on the marketplace.
This brings confidence and security to the data users.
Using \textbf{Smart Contract} technology also guarantees the rewards of the collectors.
