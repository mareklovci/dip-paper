\chapter{Conclusion and future work}\label{ch:conclusion-and-future-work}

\section{Future work}\label{sec:future-work}

In the paper I opened many topics, unable to explore them in depth.
There are many possible ways, the paper could be augmented.

\subsection{Improve URL tokenizer}\label{subsec:improve-url-tokenizer}

The tokenizer now works only for english language.
The model splitting a~text string without spaces into tokens is trained on data from english Wikipedia and other language than english will not be tokenized properly.
It would be handy to add possibility to tokenize several languages.
I can see two possibilites:

\begin{enumerate}
    \item update dataset with the language of the URL,
    \item get language from TLD (Top level domain).
\end{enumerate}

The first proposal would place high demands on the dataset management - conceivably solvable with ML model.
The second proposal would be probably very low accurate.

There is another problem with current tokenizer - it does not split correctly brand names.
To solve this problem, a~whitelist of words which should be taken as they are should be made - for example \textit{kaggle} (well known website with ML challenges and datasets), now tokenizer will not recognize as one word.

\subsection{Multiclass classification}\label{subsec:multiclass-classification}

The idea here is to improve dataset labels in order to enable multiclass classification.
The addresses could be labeled as malware (spyware, adware, ransomware, etc.), fakenews, gambling, etc., making multiclass explanations more interesting.

\subsection{Model-nonagnostic explainer}\label{subsec:model-nonagnostic-explainer}

Model agnosticism is a~strong point of LIME framework.
It was a~necessity to develop framework like this in a~model independent way.
The idea behind creating a~model-nonagnostic explainer is to \say{borrow} some data from the model and make explanations more accurate and more meaningfull.

\subsection{Explanations using n-grams}\label{subsec:explanations-using-n-grams}

The current state of LIME framework does not support n-gram and restrict the explanation terms to unigrams.
This is drawback for many possible usages and augmenting the current implementation with this functionality would be very beneficial.

\subsection{Compare explanation frameworks}\label{subsec:compare-explanation-frameworks}

The LIME is not the only framework, providing explanations of ML systems, available.
There are others like ELI5, Skate or SHAP\@.
I suggest an extension to this paper, which would serve as an status overview of those frameworks, comparison of their capabilities and use cases.

\section{Conclusion}\label{sec:conclusion}

The use of machine learning in cybersecurity is a~very complex topic becoming increasingly important.
The first contribution of this paper is in the comprehensive overview of the current state of the machine learning from three points of view:

\begin{enumerate}
    \item utilization of machine learning for systems defense used by security companies to mitigate security breaches,
    \item summary of attacks which machine learning models are facing and possible ways of defense, including security of system infrastructure and training data,
    \item ways how could an adversary take an advantage of machine learning to cause damage to an infected system.
\end{enumerate}

I reached a~conclusion that even though those methods are integral part of security systems, they cannot be relied upon completely, making human surveillance and traditional ways of threat recognition still irreplaceable.
Furthemore, despite of the growing numbers of \say{prove of concept} papers on the topic of use of machine learning for attacks, it seems that those methods are not much widespread, if at all.

There are plenty of ways how to collect URL features, making this problem very context dependent.
The next contribution is in the presented way of feature extraction from URL addresses - proving that inferring spaces from URL is possible and although the number of inferred words is very limited and in some cases very inaccurate, the classification is still attainable.
The weakness of this section is at the data.
In real application scenario, it will be impossible to train a~classifier on the datasets freely available on the internet - it is necessary to observe and extract data from real-time network traffic.
On the other hand the used classiers are very appropriate for the task, working quite well even without additional hyperparameter tuning.

The last chapter is devoted to the model analysis and it is the last contribution of this work - making sense of obtained URL classifications.
Explaining the predictions of any machine learning classifier will become more important after machine learning systems become widely used in healthcare and other decision sensitive applications.
It is quite complicated to compare \textit{explainer} output from several classifiers, insomuch as final explanation is highly dependent on used machine learning algorithm.
Overall, the results are understandable and help to accept and build trust in machine learning models.
